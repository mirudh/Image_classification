{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Dataset collection:**\n",
        "\n",
        "1. Mount your drive\n",
        "2. Upload your kaggle.json file (Download your Kaggle API token to get kaggle.json file)\n",
        "3. Download the dataset (Here, you will get the dataset as imagenet.zip)\n",
        "4. Unzip the downloaded dataset"
      ],
      "metadata": {
        "id": "_qQBId2BcNr3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqrazL5RW_UM",
        "outputId": "5b231192-8d06-4318-a4b7-b66d834baee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # Select you account, and give continue. You will be able to see the drive mounted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSAWfszsXKON",
        "outputId": "56686c48-be80-4676-cc77-22860d00737c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/ # Setting up the working directory. (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "u7uJiqD2YCEy",
        "outputId": "bca0cd0b-4a7f-4499-f426-fdc0dd4a52f9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-eb01e62e-0d4b-4ea1-92d8-f790d025662b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-eb01e62e-0d4b-4ea1-92d8-f790d025662b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Upload the download kaggle.json from your device\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqDN7ju9XOKN"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d ambityga/imagenet100 #downloading the dataset from kaggle as imagenet100.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvIgxb6uZueh"
      },
      "outputs": [],
      "source": [
        "!unzip imagenet100.zip # Unzipping the imagenet100.zip file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing the dependencies or setting up the environment**\n",
        "\n",
        "1. Create a venv, for e.g. python -m venv imagenet_vnv\n",
        "2. Install the below specified libraries/packages.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xcSmvNGacYn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the dependencies\n",
        "\n",
        "!pip install tensorflow\n",
        "!pip install tqdm\n",
        "!pip install numpy\n",
        "!pip install scikit-learn\n",
        "!pip install keras\n",
        "!pip install pandas\n",
        "!pip install opencv-python"
      ],
      "metadata": {
        "id": "aTipBqHoyGbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WISBsVdFZtL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a5893bf-2ec7-450d-9b55-edb723f88a6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-36398ed231ac>:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
            "  from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n"
          ]
        }
      ],
      "source": [
        "#Importing the libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "from zipfile import ZipFile\n",
        "import os,glob\n",
        "import cv2\n",
        "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D, Dropout, Dense,MaxPooling2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from zipfile import ZipFile"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data preparation**\n",
        "\n",
        "1. We see that our train dataset with a total of 100 classes is given in 4 different folders (train.X1, train.X2, train.x3, train.X4) with each containing 25 classes.\n",
        "2. We need to merge these folders together.\n",
        "3. Therefore, all of these datas are pushed to a new created directory, '/train' and Val.X (the validation data with 100 folders, representing 100 classes) is also pushed to a newly created directory, '/val'"
      ],
      "metadata": {
        "id": "vExiNy8OdOvm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur81pnckcYBG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Moving folders in train.X1 to train/{name of the folder}\n",
        "\n",
        "for label in os.listdir('/content/train.X1'):\n",
        "    os.makedirs(f'/content/train/{label}')\n",
        "    print('dir created')\n",
        "    for filename in os.listdir(f'/content/train.X1/{label}'):\n",
        "        shutil.copy(f'/content/train.X1/{label}/{filename}', f'/content/train/{label}/{filename}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YfRwyW-9rq8"
      },
      "outputs": [],
      "source": [
        "# Moving folders in train.X2 to train/{name of the folder}\n",
        "\n",
        "for label in os.listdir('/content/train.X2'):\n",
        "    os.makedirs(f'/content/train/{label}/{filename}')\n",
        "    print('dir created')\n",
        "    for filename in os.listdir(f'/content/train.X2/{label}'):\n",
        "        shutil.copy(f'/content/train.X2/{label}/{filename}', f'/content/train/{label}/{filename}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsvqcjyMBzaQ"
      },
      "outputs": [],
      "source": [
        "# Moving folders in train.X3 to train/{name of the folder}\n",
        "\n",
        "for label in os.listdir('/content/train.X3'):\n",
        "    os.makedirs(f'/content/train/{label}')\n",
        "    print('dir created')\n",
        "    for filename in os.listdir(f'/content/train.X3/{label}'):\n",
        "        shutil.copy(f'/content/train.X3/{label}/{filename}', f'/content/train/{label}/{filename}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXt3DYvZB91C"
      },
      "outputs": [],
      "source": [
        "# Moving folders in train.X4 to train/{name of the folder}\n",
        "\n",
        "for label in os.listdir('/content/train.X4'):\n",
        "    os.makedirs(f'/content/train/{label}')\n",
        "    print('dir created')\n",
        "    for filename in os.listdir(f'/content/train.X4/{label}'):\n",
        "        shutil.copy(f'/content/train.X4/{label}/{filename}', f'/content/train/{label}/{filename}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Moving folders in val.X to val/{name of the folder}\n",
        "\n",
        "for label in os.listdir('/content/val.X'):\n",
        "    os.makedirs(f'/content/val/{label}')\n",
        "    print('dir created')\n",
        "    for filename in os.listdir(f'/content/val.X/{label}'):\n",
        "        shutil.copy(f'/content/val.X/{label}/{filename}', f'/content/val/{label}/{filename}')"
      ],
      "metadata": {
        "id": "Yzx0yyUYmx09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Training the data**\n",
        "\n",
        "\n",
        "Here, the ImageDataGenerator() function is generally used for data augmentation. As the next step, we are generating train data and test data from the train.X1 and Val.X1 directory respectively.\n",
        "\n",
        "Parameters specified:\n",
        "\n",
        "Batch size = 32 (No.of batches/samples propagating through the network during every epoch/iteration)\n",
        "\n",
        "Class mode = Categorical (We have about 100 classes in total)\n",
        "\n",
        "target_size(224,224) (It reduces storage. It makes machine learning algorithms computationally efficient.)"
      ],
      "metadata": {
        "id": "9B1Ew3BUfbVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "trdata = ImageDataGenerator()\n",
        "traindata = trdata.flow_from_directory(directory=\"/content/train\",batch_size=32,class_mode= \"categorical\",target_size=(224,224))\n",
        "tsdata = ImageDataGenerator()\n",
        "testdata = tsdata.flow_from_directory(directory=\"/content/val\", batch_size=32, class_mode= \"categorical\",target_size=(224,224))"
      ],
      "metadata": {
        "id": "42Q4-DczfUKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the dataset is trained using 3 different models, such as:\n",
        "1. VGG16\n",
        "2. Inceptionv3\n",
        "3. ResNet50\n",
        "4. MobileNet\n",
        "\n",
        "VGG16:\n",
        "The VGG-16 is one of the most popular pre-trained models for image classification. Introduced in the famous ILSVRC 2014 Conference, it was and remains THE model to beat even today. Developed at the Visual Graphics Group at the University of Oxford, VGG-16 beat the then standard of AlexNet and was quickly adopted by researchers and the industry for their image Classification Tasks.\n",
        "\n",
        "Inception:\n",
        "Inception v3 is an image recognition model that has been shown to attain greater than 78.1% accuracy on the ImageNet dataset. The model is the culmination of many ideas developed by multiple researchers over the years.Inception v3 is a convolutional neural network for assisting in image analysis and object detection, and got its start as a module for GoogLeNet. It is the third edition of Google's Inception Convolutional Neural Network, originally introduced during the ImageNet Recognition Challenge\n",
        "\n",
        "ResNet50:\n",
        "ResNet-50 is a 50-layer convolutional neural network (48 convolutional layers, one MaxPool layer, and one average pool layer). Residual neural networks are a type of artificial neural network (ANN) that forms networks by stacking residual blocks. ResNet50 is a powerful image classification model that can be trained on large datasets and achieve state-of-the-art results. One of its key innovations is the use of residual connections , which allow the network to learn a set of residual functions that map the input to the desired output.\n",
        "\n",
        "EfficientNet:\n",
        "is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a compound coefficient\n",
        "\n"
      ],
      "metadata": {
        "id": "37R1tBTr55w-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining the model**"
      ],
      "metadata": {
        "id": "nw5sStmUKR9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using only the basic models, with changes made only to the final layer. This is because this is just a 100 calsses classification problem while these models are built to handle up to 1000 classes.\n",
        "\n",
        "\n",
        "The 'choose_model' variable can be changed to utilize any one of the above mentioned architecture to train the dataset."
      ],
      "metadata": {
        "id": "LfVLJZqHJ4An"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "choose_model = ' '\n",
        "\n",
        "if choose_model == 'VGG16':\n",
        "  from tf.keras.applications.vgg16 import VGG16\n",
        "\n",
        "  base_model = VGG16(input_shape = (224, 224, 3), # Shape of our images\n",
        "  include_top = False, # Leave out the last fully connected layer\n",
        "  weights = 'imagenet')\n",
        "\n",
        "elif choose_model == 'Inceptionv3':\n",
        "  from tf.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "  base_model = InceptionV3(input_shape = (224, 224, 3),\n",
        "                          include_top = False,\n",
        "                          weights = 'imagenet')\n",
        "\n",
        "elif choose_model == 'ResNet50':\n",
        "  from tf.keras.applications import ResNet50\n",
        "\n",
        "  base_model = InceptionV3(input_shape = (224, 224, 3),\n",
        "                          include_top = False,\n",
        "                          weights = 'imagenet')\n",
        "\n",
        "elif choose_model == 'efficientNet':\n",
        "  import efficientNet.keras as efn\n",
        "  base_model = efn.EfficientNetB0( input_shape = (224, 224, 3),\n",
        "                                 include_top = False,\n",
        "                                 weights = 'imagenet')\n"
      ],
      "metadata": {
        "id": "g44blU4QfqyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we don’t have to train all the layers, we make them non_trainable"
      ],
      "metadata": {
        "id": "cgrZVJmZJ_m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "0sy5s_NKgCuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We will then build the last fully-connected layer. I have just used the basic settings, but feel free to experiment with different values of dropout, and different Optimisers, activation functions and learning rate.\n",
        "\n",
        "The flatten layer serves the purpose of reshaping the output of the preceding layer into a one-dimensional vector, which can then be fed into subsequent fully connected layers\n",
        "\n",
        "A dense layer is a layer where each neuron is connected to every neuron in the previous layer. In other words, the output of each neuron in a dense layer is computed as a weighted sum of the inputs from all the neurons in the previous layer. Here, we have 1024 neurons/units present in the dense layer.\n",
        "\n",
        "The Dropout Layer. Another typical characteristic of CNNs is a Dropout layer. The Dropout layer is a mask that nullifies the contribution of some neurons towards the next layer and leaves unmodified all others. Here we dropout 0.5 units.\n",
        "\n",
        "Finally the output layer with 100 neurons specifying 100 classes."
      ],
      "metadata": {
        "id": "6qx_Jru-Knzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = Flatten()(base_model.output)\n",
        "\n",
        "# Add a fully connected layer with 1024 hidden units and ReLU activation\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "\n",
        "# Add a dropout rate of 0.5\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "# Add a final sigmoid layer with 100 node for classification output\n",
        "x = Dense(100, activation='softmax')(x)\n",
        "\n",
        "model = tf.keras.models.Model(base_model.input, x)\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.  Adam(lr=0.001), loss = 'categorical_crossentropy',metrics = ['acc'])"
      ],
      "metadata": {
        "id": "sGZ5hR8HgIug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summarizing the model**"
      ],
      "metadata": {
        "id": "Mt6gQjTBLHX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "JBj5LNONgrEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now build the final model based on the training and validation sets we created earlier. Please note to use the original directories itself."
      ],
      "metadata": {
        "id": "K3uihXE2K1t_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "checkpoint = ModelCheckpoint(\"Model.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')\n",
        "hist = model.fit(steps_per_epoch=len(traindata),generator=traindata, validation_data= testdata, validation_steps=len(testdata),epochs=5,callbacks=[checkpoint,early])"
      ],
      "metadata": {
        "id": "gkLz_5frgusQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation Metrics**\n",
        "1. Training loss\n",
        "2. Training Accuracy\n",
        "3. Validation Loass\n",
        "4. Validation accuracy\n",
        "\n",
        "These values are obtained at each and every epoch. These are later on plotted for better visualization\n"
      ],
      "metadata": {
        "id": "tzq8zT96jG4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Graph to check loss and accuracy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(hist.history[\"acc\"])\n",
        "plt.plot(hist.history['val_acc'])\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title(\"model accuracy\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend([\"Accuracy\",\"Validation Accuracy\",\"loss\",\"Validation Loss\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "juX4XIxog3Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision:\n",
        "\n",
        "Precision is one indicator of a machine learning model's performance – the quality of a positive prediction made by the model. Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives)\n",
        "\n",
        "\n",
        "Recall:\n",
        "\n",
        "Recall is a metric that measures how often a machine learning model correctly identifies positive instances (true positives) from all the actual positive samples in the dataset. You can calculate recall by dividing the number of true positives by the number of positive instances\n",
        "\n",
        "\n",
        "F1 Score:\n",
        "\n",
        "The F1 score or F-measure is described as the harmonic mean of the precision and recall of a classification model. The two metrics contribute equally to the score, ensuring that the F1 metric correctly indicates the reliability of a model\n",
        "\n",
        "The classification report is generated for the same."
      ],
      "metadata": {
        "id": "jj7v4FgRLtI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision, recall, f1 score\n",
        "Y_pred = model.predict(testdata, testdata.samples / 32)\n",
        "val_preds = np.argmax(Y_pred, axis=1)\n",
        "import sklearn.metrics as metrics\n",
        "val_trues =testdata.classes\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(val_trues, val_preds))"
      ],
      "metadata": {
        "id": "1fPvGDSBiQPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A confusion matrix is a performance evaluation tool in machine learning, representing the accuracy of a classification model. It displays the number of true positives, true negatives, false positives, and false negatives"
      ],
      "metadata": {
        "id": "2V-3knA1MPTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "\n",
        "Y_pred = model.predict(testdata, testdata.samples / 32)\n",
        "val_preds = np.argmax(Y_pred, axis=1)\n",
        "val_trues =testdata.classes\n",
        "cm = metrics.confusion_matrix(val_trues, val_preds)\n",
        "cm"
      ],
      "metadata": {
        "id": "dYEsMpo_kAd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediction using new image**"
      ],
      "metadata": {
        "id": "_OXhmR-pkndq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create new file test.py and run this file\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "#load saved model\n",
        "model = load_model(\"Model.h5\") # or model = tf.keras.applications.VGG16(weights='imagenet', input_shape=(128, 128, 3))\n",
        "img_path = '/path/to/input_image.jpg' # Input the image path\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "preds=model.predict(x)\n",
        "# create a list containing the class labels\n",
        "class_labels=[]\n",
        "f = open('Labels.json')\n",
        "data = json.load(f)\n",
        "for dir in glob.glob('/content/Val/*'):\n",
        "  label = dir.split('/')[-1]\n",
        "  class_labels.append(data[label])\n",
        "# find the index of the class with maximum score\n",
        "pred = np.argmax(preds, axis=-1)\n",
        "# print the label of the class with maximum score\n",
        "print(class_labels[pred[0]])"
      ],
      "metadata": {
        "id": "LX6QUqqkk3Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SHAP Values generation**\n",
        "\n",
        "SHAP (SHapley Additive exPlanations) values are a way to explain the output of any machine learning model. It uses a game theoretic approach that measures each player's contribution to the final outcome.  Shapley values are a widely used approach from cooperative game theory that come with desirable properties"
      ],
      "metadata": {
        "id": "ij-L11FPrX6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "class_labels=[]\n",
        "f = open('Labels.json')\n",
        "data = json.load(f)\n",
        "for dir in glob.glob('/content/val/*'):\n",
        "  label = dir.split('/')[-1]\n",
        "  class_labels.append(data[label])\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = load_model('Model.h5')\n",
        "\n",
        "# Define a function to preprocess the images\n",
        "def preprocess_image(image_path):\n",
        "    img = load_img(image_path, target_size=(128, 128))\n",
        "    x = img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    return x\n",
        "\n",
        "# Single image implementation\n",
        "image_path = '/content/val/n01440764/ILSVRC2012_val_00000293.JPEG'\n",
        "image = preprocess_image(image_path)\n",
        "\n",
        "predictions = model.predict(image)\n",
        "predicted_class_index = np.argmax(predictions)\n",
        "confidence = predictions[0, predicted_class_index]\n",
        "predicted_class_name = class_labels[predicted_class_index]\n",
        "print(f\"Prediction for {image_path}: {predicted_class_name} ({confidence * 100:.2f}%)\")\n",
        "\n",
        "\n",
        "# Create a masker object for the PartitionExplainer\n",
        "masker = shap.maskers.Image(\"inpaint_telea\", image[0].shape)\n",
        "\n",
        "# Compute SHAP values using PartitionExplainer\n",
        "explainer = shap.Explainer(model, masker,output_names=class_labels)\n",
        "shap_values = explainer(\n",
        "    image, max_evals=500, batch_size=50, outputs=shap.Explanation.argsort.flip[:8]\n",
        ")\n",
        "shap.image_plot(shap_values)\n",
        "\n",
        "# Complete folder implementation\n",
        "\n",
        "# folder_path = 'path/to/folder'\n",
        "# for i in os.listdir(folder_path):\n",
        "#   image_path = f'path/to/folder/{i}'\n",
        "#   image_path = '/content/val/n01440764/ILSVRC2012_val_00000293.JPEG'\n",
        "#   image = preprocess_image(image_path)\n",
        "\n",
        "#   predictions = model.predict(image)\n",
        "#   predicted_class_index = np.argmax(predictions)\n",
        "#   confidence = predictions[0, predicted_class_index]\n",
        "#   predicted_class_name = class_labels[predicted_class_index]\n",
        "#   print(f\"Prediction for {image_path}: {predicted_class_name} ({confidence * 100:.2f}%)\")\n",
        "\n",
        "\n",
        "#   # Create a masker object for the PartitionExplainer\n",
        "#   masker = shap.maskers.Image(\"inpaint_telea\", image[0].shape)\n",
        "\n",
        "#   # Compute SHAP values using PartitionExplainer\n",
        "#   explainer = shap.Explainer(model, masker,output_names=class_labels)\n",
        "#   shap_values = explainer(\n",
        "#       image, max_evals=500, batch_size=50, outputs=shap.Explanation.argsort.flip[:4]\n",
        "#   )\n",
        "#   shap.image_plot(shap_values)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nJ5TrqANHE4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Post Model Quantization**\n",
        "\n",
        "Model quantization is vital for deploying large AI models on resource-constrained devices. Quantization levels, like 8-bit or 16-bit, reduce model size and improve efficiency."
      ],
      "metadata": {
        "id": "kd88dZ5P0VvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the Keras model\n",
        "model = tf.keras.models.load_model('vgg16.h5')\n",
        "\n",
        "# Initialize the TFLiteConverter with the Keras model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# Convert the model to TensorFlow Lite format\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TensorFlow Lite model to a file\n",
        "with open('model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model) # model.tflite will be saved in your working directory\n",
        "\n",
        "print('completed')"
      ],
      "metadata": {
        "id": "ftFP5YSD0DO2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
